
* add private[aspen] getHostPointer, storageDevice, and pool
* implement getHost/Pool/SD in terms of the get Pointer
* Rather than getTree, just create a val for each TKVL tree in the Radicle
  simplifies code

Modify StoragePool.Config to:
  1. drop numberOfStores (use stores.length)
  2. rename storeHosts to stores
  3. change stores to Array[StoreState]
  4. add case StoreState(HostId, StorageDeviceId)
  5. remove SimpleAspenClient.updateHost to modify pool config?
     - Move to StoreManager?

For ZNetwork
  - If have store -> HostId mapping, send directly
  - If not, Lookup via AspenClient
    When Future resolves, enter synchronized block
      1. if HostId not in pendingConnectionsSet, create new socket and update set
      2. encuque AddHostConnection(zmqsocket and stuff)
      3. enqueue the original message to be sent
  - When get AddHostConnection message:
      1. add new host connection to map


Add StorageDeviceTree to radicle
Add BootstrapConfigKey and yaml value to radicle

Bootstrap
  - Pass in the bootstrap Host config object
    * Should use generated Host ID and Storage Device
  - Always create all the bootstrap stores within the bootstrap StorageDevice
    * They have to be transferred out

Create ConfigStateManager
  - Encapsulates the PoolConfig, Host, StorageDevice implementation
  - Takes client & bootstrapConfig args

StoreManager
  - Switch to managing drives that contain stores
  - For now use host_config.yaml
     - Lists network ports
     - List path to each StorageDevice

Create HostResolver
  - Takes AspenClient as argument
  - Takes BootstrapConfig as argument
     - Allows bootstrap pool storeId and host lookups without consulting client
  - Responsible for looking up HostId -> Host and StoreId -> Host
  - Handles caching
  - member variable of client?
    - Would make it easy for cache invalidation messages to come in from external
      sources

Add RefreshBootstrapConfig CnC message
  - Lookup bootsrap PoolConfig
  - Lookup host info for each storeId
     - Dump hosts to top of yaml file
     - Dump storeId -> hostId mapping
  - Store this in BootstrapConfigKey in Radicle

After migrating bootstrap store, send RefreshBootstrapConfig CnC message to each
host.
  - Will need a way to propagate this across all clients.
  - gossip protocol?
  - clients periodically poll a host for latest BootstrapConfig hash?

Update ZMQNetwork to use the NetworkResolver or maybe just client
  - Set resolver after creation (must create network instance before client)
  - Can probably just set the resolver to null in constructor
    - Will be set before first use so should be fine

ServerMessenger
  - getCnCFrontend(hostId: HostId): Future[CnCFrontend]

Add StorageDeviceState
  - contains host uuid
  - contains StoreIds and current state
    - Initializing, TransferringIn, TransferringOut, or Active

Add HostState
  - Contains network endpoint config
  - Contains list of StorageDevice UUIDs

Host walks all StorageDevices
  1. Load all stores on all devices
    * Ignore stores that have the 'transferring-out' marker file
  2. For each storage device
    storageDeviceStateCheck():
    * Lookup storage device state
      * If state.host-id != current host-id
         In single Tx:
            Update StorageState object to point to new host
            Update the PoolConfig for each store to point to new host
      * For each store
         - If Initializing, crete store and update storage device state to Active
         - If TransferringIn,
            - Check to see if store is already loaded.
              - In Tx
                update state to Active
                update PoolConfig to reflect new host and device
            - Else, if not already transferring, start

When transfer ends successfully
  * Remove 'transferring-out' marker file
  * os.rename from <device-root>/transferring-in/<storeId> to <device-root>/<storeId>
  * Load store
  * trigger storge device state check process mentioned above
     (Process will see that the store is loaded and will update the state)

To begin transfer:
  * Verify source and host devices are different (would be a mess otherwise)
  * In Tx
    update source drive state to set TransferringOut
    update target drive state to TransferringIn
  * Use CnC to ask destination host to trigger a deviceStateCheck

On CnC transfer out request:
  * If active, take store offline
  * generate crl log
  * create 'transferring-out' marker file

To create new pool:
  1. Identify all the newStoreId, StorageDevice pairings
    - Collect into StorageDevice, List[newStoreIds]
  2. in single Tx
    * Create PoolConfig object
    * insert into TKVL
    * Update each StorageDeviceState to include new storeIds in Initializing state
  3. For each StorageDevice touched, use CnC to trigger a deviceStateCheck

-----------------------------------------------------------

TODO:
  * Globally replace Future.successful(()) with Future.unit

# Setting up intellij to use a JDK installed by homebrew:
# homebrew install openjdk
# Launch Intellj and open project folder
# When a JDK is not detected:
    Go to File -> Project Structure
    In SDK option, add an SDK
    Use the following path to the SDK:
        /opt/homebrew/opt/openjdk/libexec/openjdk.jdk/Contents/Home

# Run only a single test suite
sbt 'testOnly org.aspen_ddp.aspen.common.DataBufferSuite'

# Run single test. Where "min/max" identifies a test containing this substring
sbt 'testOnly *KeyValueObjectReaderSuite -- -z min/max'

# Testing commands

./t bootstrap demo/bootstrap_config.yaml

./t node demo/bootstrap_config.yaml demo/node_a.yaml
./t node demo/bootstrap_config.yaml demo/node_b.yaml
./t node demo/bootstrap_config.yaml demo/node_c.yaml

sudo launchctl start com.apple.rpcbind
./t nfs demo/cfg.yaml demo/log4j-conf.xml

On Client:
    ssh rakis@192.168.64.2
    sudo
    umount -f /mnt; mount -v -t nfs4 -o "vers=4.1" 192.168.56.1:/ /mnt

./t new-pool demo/log4j-conf.xml demo/bootstrap_config.yaml new_pool_name replication 3 2 2 node_a,node_b,node_c

./t transfer-store demo/log4j-conf.xml demo/bootstrap_config.yaml 3f1ca5f7-74be-405d-aa48-739fc23651a4:2 node_a


-------------------------------------------------------------------------------
- Old Testing Commands
-------------------------------------------------------------------------------

# Building for demo appliction
# run "sbt pack"
# then invoke ./target/pack/bin/demo

sbt compile && rm -r demo/node_* && ./t bootstrap demo/cfg.yaml

Bootstrap Procedure:
    sbt pack
    rm -r ~/temp/aspen/*
    rm -r demo/*log
    ./target/pack/bin/demo bootstrap demo/t.yaml
    emacs demo/cfg.yaml
    ./target/pack/bin/demo amoeba demo/cfg.yaml demo/amoeba-log4j.yaml

  For each node:
    ./target/pack/bin/demo node demo/cfg.yaml node_a

  On client:
    ssh rakis@192.168.64.2
    sudo
    umount -f /mnt; mount -v -t nfs4 -o "vers=4.1" 192.168.56.1:/ /mnt

NFS Cache Clearing:
  # To free pagecache
  echo 1 > /proc/sys/vm/drop_caches

  # To free dentries and inodes
  echo 2 > /proc/sys/vm/drop_caches

  # To free pagecache, dentries and inodes
  echo 3 > /proc/sys/vm/drop_caches


Pre sbt-pack demo launch script:
#!/bin/bash

# The CLASSPATH content is generated by sbt via running the "export runtime:fullClasspath" command
#
export CLASSPATH="<sbt content>"

exec java -cp $CLASSPATH com.ibm.aspen.demo.Main "$@"

---------------------------------------------------------------------------
- TODO
-

AllocateResponse
    - Add error string to message on failure
    - Handle allocation failure
    
KeyRevisionGuard
    - Add Zeroed DoesNotExist variable
    - Test for zeroed UUID in Tx, use this for does not exist requirement

Transaction Finalizers
    - Improve error handling
    - Move to dedicated registry rather than the generic one
        - Should be argument to AmoebaClient for user-defined FAs
    - Consider moving this registry to Server side only. That's where they're used

AllocationDriver
    - Detect stalled allocations and force commit failure
    - Recover from successful completions

SimpleCRL
    Problem if LogEntry exceeds the Maximum stream size. Can modify LogEntry to skip writes until its size
    is once again less than the size of a log file. Need to allow tx/alloc deletes to remove entries from
    the LogEntry.
    - Move completionHandler into the LogContent base class
    - Iterate over all Tx and Alloc to call completion handlers
    - Modify LogEntry to remove Tx and Alloc instances when deletions are made

FIXME - When setting Maximum on KeyValue object, we must check to ensure that no keys exceeding that
        maximum will be present when transaction ends. Must run through adds and deletes to local keys
        to determine this.
   |-> Prevents hole where KV additions are made after a read which results in a split. could leave
       behind keys with value > maximum


